---
layout: post
title:  "On the StyleGAN"
date:   2019-12-15 18:00:00
categories: main

---

# Getting Started

It is no surprise that a lot of computing power will be needed to generate large generated images.

Should only technical users be able to have fun with new emerging technologies? Most likely this will be the case in the beginning, but this will for sure change as we slowly mature both the algorithms and necessary technologies in order to access to these.

# StyleGAN

## Progressive Growing

## The Generator

### $Z$ and $W$

###

## The Discriminator



# On Threads

But how to apply all of this to a concrete project? While we can of course download the pretrained models done by the official authors, it is also fun to try our own experiments and monumentally fail at every step of the way, until we achieve something that captivates our eyes.

As [Helena Sarin notes](https://twitter.com/glagolista/status/1200819679209627648?s=20), a gut feeling is still needed whenever deciding when a generated image, video,

At the end, while the work I present here is certainly beautiful, I still am far more enamored with what I've [previously generated](https://blog.diegoporres.com/main/2019/07/17/UnsupervisingArt/) with *lower class* GANs ([DCGAN](https://arxiv.org/abs/1511.06434)). [Bigger GANs](https://www.artnome.com/news/2018/11/14/helena-sarin-why-bigger-isnt-always-better-with-gans-and-ai-art) or better algorithms won't necessarily bring forth what we wish to express, but they should be explored in order to determine firsthand if what they capture in their latent space is close to what we wish to convey.

<iframe width="560" height="315" src="https://www.youtube.com/embed/4nktYGjSVHg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

{% include disqus.html %}
